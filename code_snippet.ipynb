{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download dataset via kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# path = kagglehub.dataset_download(\"mitishaagarwal/patient\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give a variable to store the path\n",
    "- Read the CSV from the path\n",
    "- Display the first 5 rows of data to test if successfully read the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/soongjun/.cache/kagglehub/datasets/mitishaagarwal/patient/versions/3/dataset.csv\"\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the number of rows and columns\n",
    "- Display the columns' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (91713, 85) \n",
      "\n",
      "Columns: Index(['encounter_id', 'patient_id', 'hospital_id', 'age', 'bmi',\n",
      "       'elective_surgery', 'ethnicity', 'gender', 'height', 'icu_admit_source',\n",
      "       'icu_id', 'icu_stay_type', 'icu_type', 'pre_icu_los_days', 'weight',\n",
      "       'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_post_operative',\n",
      "       'arf_apache', 'gcs_eyes_apache', 'gcs_motor_apache',\n",
      "       'gcs_unable_apache', 'gcs_verbal_apache', 'heart_rate_apache',\n",
      "       'intubated_apache', 'map_apache', 'resprate_apache', 'temp_apache',\n",
      "       'ventilated_apache', 'd1_diasbp_max', 'd1_diasbp_min',\n",
      "       'd1_diasbp_noninvasive_max', 'd1_diasbp_noninvasive_min',\n",
      "       'd1_heartrate_max', 'd1_heartrate_min', 'd1_mbp_max', 'd1_mbp_min',\n",
      "       'd1_mbp_noninvasive_max', 'd1_mbp_noninvasive_min', 'd1_resprate_max',\n",
      "       'd1_resprate_min', 'd1_spo2_max', 'd1_spo2_min', 'd1_sysbp_max',\n",
      "       'd1_sysbp_min', 'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min',\n",
      "       'd1_temp_max', 'd1_temp_min', 'h1_diasbp_max', 'h1_diasbp_min',\n",
      "       'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',\n",
      "       'h1_heartrate_max', 'h1_heartrate_min', 'h1_mbp_max', 'h1_mbp_min',\n",
      "       'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_resprate_max',\n",
      "       'h1_resprate_min', 'h1_spo2_max', 'h1_spo2_min', 'h1_sysbp_max',\n",
      "       'h1_sysbp_min', 'h1_sysbp_noninvasive_max', 'h1_sysbp_noninvasive_min',\n",
      "       'd1_glucose_max', 'd1_glucose_min', 'd1_potassium_max',\n",
      "       'd1_potassium_min', 'apache_4a_hospital_death_prob',\n",
      "       'apache_4a_icu_death_prob', 'aids', 'cirrhosis', 'diabetes_mellitus',\n",
      "       'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma',\n",
      "       'solid_tumor_with_metastasis', 'apache_3j_bodysystem',\n",
      "       'apache_2_bodysystem', 'Unnamed: 83', 'hospital_death'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape:\", data.shape, \"\\n\")\n",
    "print(\"Columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop empty column by index\n",
    "- Count the number of rows and columns to ensure the column is removed\n",
    "- Display the new columns' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (91713, 82) \n",
      "\n",
      "Columns: Index(['patient_id', 'age', 'bmi', 'elective_surgery', 'ethnicity', 'gender',\n",
      "       'height', 'icu_admit_source', 'icu_id', 'icu_stay_type', 'icu_type',\n",
      "       'pre_icu_los_days', 'weight', 'apache_2_diagnosis',\n",
      "       'apache_3j_diagnosis', 'apache_post_operative', 'arf_apache',\n",
      "       'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache',\n",
      "       'gcs_verbal_apache', 'heart_rate_apache', 'intubated_apache',\n",
      "       'map_apache', 'resprate_apache', 'temp_apache', 'ventilated_apache',\n",
      "       'd1_diasbp_max', 'd1_diasbp_min', 'd1_diasbp_noninvasive_max',\n",
      "       'd1_diasbp_noninvasive_min', 'd1_heartrate_max', 'd1_heartrate_min',\n",
      "       'd1_mbp_max', 'd1_mbp_min', 'd1_mbp_noninvasive_max',\n",
      "       'd1_mbp_noninvasive_min', 'd1_resprate_max', 'd1_resprate_min',\n",
      "       'd1_spo2_max', 'd1_spo2_min', 'd1_sysbp_max', 'd1_sysbp_min',\n",
      "       'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min', 'd1_temp_max',\n",
      "       'd1_temp_min', 'h1_diasbp_max', 'h1_diasbp_min',\n",
      "       'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',\n",
      "       'h1_heartrate_max', 'h1_heartrate_min', 'h1_mbp_max', 'h1_mbp_min',\n",
      "       'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_resprate_max',\n",
      "       'h1_resprate_min', 'h1_spo2_max', 'h1_spo2_min', 'h1_sysbp_max',\n",
      "       'h1_sysbp_min', 'h1_sysbp_noninvasive_max', 'h1_sysbp_noninvasive_min',\n",
      "       'd1_glucose_max', 'd1_glucose_min', 'd1_potassium_max',\n",
      "       'd1_potassium_min', 'apache_4a_hospital_death_prob',\n",
      "       'apache_4a_icu_death_prob', 'aids', 'cirrhosis', 'diabetes_mellitus',\n",
      "       'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma',\n",
      "       'solid_tumor_with_metastasis', 'apache_3j_bodysystem',\n",
      "       'apache_2_bodysystem', 'hospital_death'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(data.columns[-2], axis=1)\n",
    "data = data.drop(columns = [\"encounter_id\", \"hospital_id\"])\n",
    "\n",
    "print(\"Dataset shape:\", data.shape, \"\\n\")\n",
    "print(\"Columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the total columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns with missing values: 74\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().mean() * 100\n",
    "\n",
    "missing_summary = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Total columns with missing values: {missing_summary.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set a random seed to ensure the randomness is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate the columns into numerical and categorical\n",
    "- Handle missing values in numerical and categorical seperately\n",
    "- Numerical will fill in missing values with medium values\n",
    "- Categorical will fill in missing values with random choice from non-missing values\n",
    "- Check if missing values is remained exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "for col in numerical_columns:\n",
    "    if data[col].isnull().sum() > 0:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if data[col].isnull().sum() > 0:\n",
    "        non_missing_values = data[col].dropna().unique()\n",
    "        \n",
    "        data[col] = data[col].apply(\n",
    "            lambda x: np.random.choice(non_missing_values) if pd.isnull(x) else x\n",
    "        )\n",
    "\n",
    "missing_values_after_imputation = data.isnull().sum().sum()\n",
    "print(f\"Total missing values after imputation: {missing_values_after_imputation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the data types to identify categorical columns\n",
    "- Apply one-hot Encoding to to categorical columns\n",
    "- Count the number of rows and columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: ['ethnicity', 'gender', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem'] \n",
      "\n",
      "Shape after encoding: (91713, 113) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(\"Categorical Columns:\", categorical_columns, \"\\n\")\n",
    "# print(\"Data after one-hot encoding:\", \"\\n\")\n",
    "# print(df_encoded.head(), \"\\n\")\n",
    "print(\"Shape after encoding:\", data_encoded.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify the numerical columns\n",
    "- Remove the target variables\n",
    "- Normalise or scale numerial features with one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after scaling: (91713, 113) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = data_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numerical_columns.remove('hospital_death')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# print(\"Data after scaling:\", \"\\n\")\n",
    "# print(data_encoded.head(), \"\\n\")\n",
    "print(\"Shape after scaling:\", data_encoded.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate the encoded data into X and y\n",
    "- Split the data into training, validation, and testing set\n",
    "- Data spliting ratio is 80:10:10\n",
    "- Apply feature scaling after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (73370, 111) \n",
      "\n",
      "Validation set shape: (9171, 111) \n",
      "\n",
      "Test set shape: (9172, 111) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_encoded.drop(columns=['hospital_death', 'patient_id'])\n",
    "y = data_encoded['hospital_death']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train_scaled.shape, \"\\n\")\n",
    "print(\"Validation set shape:\", X_val_scaled.shape, \"\\n\")\n",
    "print(\"Test set shape:\", X_test_scaled.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import external Python file which is the model architecture\n",
    "- Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_architecture import MLPModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set initial hyperparameters\n",
    "- Call the model from external Python file\n",
    "- Display the architecture of the model\n",
    "- Apply Binary Cross Entropy Loss\n",
    "- Apply Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=111, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_layers = [128, 64]\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "model = MLPModel(input_dim=input_dim, hidden_layers=hidden_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
